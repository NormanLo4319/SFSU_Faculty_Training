for (i in 2:12){
result <- trycatch(cor(Balance, Credit[i])),
error=function(e)
paste("NA")
print(result)
corr <- append(corr, result)
}
# Create a for loop to get all the correlation between Balance and other variables.
corr <- c()
for (i in 2:12){
result <- trycatch(cor(Balance, Credit[i])),
error=function(e)
paste("NA"))
print(result)
corr <- append(corr, result)
}
# Create a for loop to get all the correlation between Balance and other variables.
corr <- c()
for (i in 2:12){
result <- trycatch(cor(Balance, Credit[i]),
error=function(e)
paste("NA"))
print(result)
corr <- append(corr, result)
}
for (i in 2:12){
result <- tryCatch(cor(Balance, Credit[i]),
error=function(e)
paste("NA"))
print(result)
corr <- append(corr, result)
}
corr
# Create a for loop to get all the correlation between Balance and other variables.
corr <- c()
for (i in 2:12){
result <- tryCatch(cor(Balance, Credit[i]), error=function(e)
paste("NA"))
print(result)
corr <- append(corr, result)
}
corr
View(Credit)
# Let's try the forward selection method
fit1 <- lm(Balance~Rating, data=Credit)
summary(fit1)
# Let's try to add the second highest correlated variable Limit
fit2 <- lm(Balance~Rating+Limit, data=Credit)
summary(fit2)
# Now, let's try to add the third highest correclated variable Income
fit3 <- lm(Balance~Rating+Income, data=Credit)
summary(fit3)
# Now, let's try adding Card to the model
fit4 <- lm(Balance~Rating+Income+Card, data=Credit)
# Now, let's try adding Card to the model
fit4 <- lm(Balance~Rating+Income+Cards, data=Credit)
summary(fit4)
# Let's try adding gender to the model.
fit5 <- lm(Balance~Rating+Income+Gender, data=Credit)
summary(fit5)
# Let's try adding Student to the model.
fit6 <- lm(Balance~Rating+Income+Student, data=Credit)
summary(fit6)
# Let's try Married to the model
fit7 <- lm(Balance~Rating+Income+Student+Married, data=Credit)
summary(fit7)
# Let's try adding Ethnicity to the model.
fit8 <- lm(Balance~Rating+Income+Student+Ethnicity, data=Credit)
summary(fit8)
final_fit <- lm(Balance~Rating+Income+Student, data=Credit)
plot(Balance, Rating)
plot(Balance, Income)
plot(Balance, Student)
plot(Rating, Balance)
plot(Income, Balance)
plot(Student, Balance)
# Plotting the regression line
abline(final_fit, lwd=3, col="red")
# Check again the data between Balance and each of the variable in the model
plot(Rating, Balance)
# Plotting the regression line
abline(final_fit, lwd=3, col="red")
# Plotting the regression line
abline(fit6, lwd=3, col="blue")
# Divides the plotting region into a 2x2 grid of panels.
par(mfrow=c(2,2))
plot(fit6)
# We can also use the residuals() function to compute the residuals from the fitted model.
plot(predict(fit6), residuals(fit6))
plot(predict(fit6), rstudent(fit6))
# We can also compute the Leverage statistics for any number of predictors using the hatvalues() function
plot(hatvalues(lm.fit))
# We can also compute the Leverage statistics for any number of predictors using the hatvalues() function
plot(hatvalues(fit6))
# We can also find out the largest leverage statistic observation in the model.
which.max(hatvalues(fit6))
# One of the common test for multicollinear for multivariable linear regression is VIF
library(car)
vif(lm.fit)
vif(fit6)
predict(fit6, ,interval = "confidence",level = .99)
predict(fit6, interval="confidence", level=0.99)
# At this point, we can try to make a prediction of Balance based on our fitted model.
predict(fit6, interval="confidence", level=0.99, se.fit=TRUE)
new_data <- data.frame(rating=c(680, 480, 280))
new_data
new_data <- data.frame(rating<-c(680, 480, 280), income<-c(55, 80, 130))
new_data
new_data <- (680, 80, 1)
predict(fit6, new_data, interval="confidence", level=0.99)
new_data <- data.frame("rating"<-c(680, 480, 280), "income"<-c(55, 80, 130), 'student'<-c("Yes", "Yes", "No"))
new_data
View(new_data)
View(new_data)
rm(new_data)
new_data <- data.frame("rating"<-c(680, 480, 280), "income"<-c(55, 80, 130), 'student'<-c("Yes", "Yes", "No"))
new_data
View(new_data)
rm(new_data)
new_data <- data.frame("rating"<-c(680, 480, 280), "income"<-c(55, 80, 130), "student"<-c("Yes", "Yes", "No"))
str(x)
View(new_data)
str(new_data)
new_data
rm(new_data)
new_data <- data.frame("r"<-c(680, 480, 280), "i"<-c(55, 80, 130), "s"<-c("Yes", "Yes", "No"))
str(new_data)
new_data
r <- c(680, 480, 280)
i <- c(55, 80, 130)
s <- c("Yes", "Yes", "No")
rm(new_data)
new_data <- data.frame(r,i,s)
str(new_data)
new_data
predict(fit6, new_data, interval="confidence", level=0.99)
# Import ISLR library
require(ISLR)
# Importing xlsx package for exporting data set to xlsx format
install.packages('xlsx')
library("xlsx")
names(Default)
write.xlsx(Default, 'Data\\Textbook_Data\\Default.xlsx', row.names=FALSE)
write.xlsx(Default, file='Data\\Textbook_Data\\Default.xlsx', row.names=FALSE)
library("xlsx")
library("xlsx")
Sys.setenv(JAVA_HOME='C:\\Program Files\\Java\\jre7')
library(rJava)
library("xlsx")
names(Default)
# Importing xlsx package for exporting data set to xlsx format
install.packages('xlsx')
library("xlsx")
# Importing xlsx package for exporting data set to xlsx format
install.packages('xlsx',INSTALL_opts="--no-multiarch")
library("xlsx")
Sys.setenv(JAVA_HOME='C:\\Program Files\\Java\\jre7')
Sys.setenv(JAVA_HOME='C:\\Program Files (x86)\\Java\\jre7')
library(rJava)
install.packages(rJava)
install.packages("rJava")
library(rJava)
Sys.setenv(JAVA_HOME='C:\\Program Files\\Java\\jre7')
Sys.setenv(JAVA_HOME='C:\\Program Files (x86)\\Java\\jre7')
library(rJava)
library("xlsx")
names(Default)
# Import ISLR library
require(ISLR)
names(Default)
write.xlsx(Default, 'Data\\Textbook_Data\\Default.xlsx', row.names=FALSE)
# Call out the Smarket data set
names(Smarket)
# print summary of the data set
summary(Smarket)
# Export dataset to csv file
write.csv(Smarket, 'Data\\Textbook_Data\\Smarket.csv', row.names=FALSE)
# Plot the pairs correlation of the data
pairs(Smarket,col=Smarket$Direction)
# Logistic Regression
# Use the General Linear Model function to build the logistic regression model.
glm.fit <- glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,
data=Smarket,family=binomial)
# Print the summary of the fitted model
summary(glm.fit)
# Generate the probabilyt function based on the fitted model for prediction
glm.probs <- predict(glm.fit,type="response")
# Use the probability function to predict the first five observations in the data
# It will return the probability for going up the next day
glm.probs[1:5]
# We can also use the ifelse() function to change the return values
# If the probabiliyt is over 50%, it will return "Up", else it will return "Down"
glm.pred <- ifelse(glm.probs>0.5,"Up","Down")
# Compare the return values to the previous
# As you observed, the probability function is nested to this new prediction function
glm.pred[1:5]
# Check the model prediction to the actual data
table(glm.pred,Direction)
# We can attach the Smarket data set for convenience
attach(Smarket)
# Check the model prediction to the actual data
table(glm.pred,Direction)
# We can also check the error of the model prediction by reversing the measure
mean(glm.pred != Direction)
# For better accuracy check, we are going to split our data into training and testing sets
# Make training and testing set
# Training data will be the data with year before 2005
train <- Year<2005
# Use the glm() function to build the logistic regression model on the training data
glm.fit <- glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,
data=Smarket,family=binomial, subset=train)
# Create the probability function by using the fitted model
glm.probs <- predict(glm.fit,newdata=Smarket[!train,],type="response")
# Create the prediction function by defining the results with threshold 50% or above
glm.pred <- ifelse(glm.probs >0.5,"Up","Down")
# Compare the prediction with the testing data
Direction.2005 <- Smarket$Direction[!train]
# Present the comparison in a matrix table
table(glm.pred,Direction.2005)
# Calculate the accuracy for the prediction
mean(glm.pred == Direction.2005)
# We can also find the error rate for the prediction
mean(glm.pred != Direction.2005)
# In the previous model, we found that the lag1 and lag2 are the best predictors in the model
# Let's build a smaller model with just lag1 and lag2 for the logistic regression model
glm.fit <- glm(Direction~Lag1+Lag2,
data=Smarket,family=binomial, subset=train)
glm.probs <- predict(glm.fit,newdata=Smarket[!train,],type="response")
glm.pred <- ifelse(glm.probs >0.5,"Up","Down")
table(glm.pred,Direction.2005)
mean(glm.pred == Direction.2005)
mean(glm.pred != Direction.2005)
# Import ISLR library
require(ISLR)
# Importing xlsx package for exporting data set to xlsx format
install.packages('xlsx')
library("xlsx")
names(Default)
write.xlsx(Default, 'Data\\Textbook_Data\\Default.xlsx', row.names=FALSE)
# Call out the Smarket data set
names(Smarket)
# print summary of the data set
summary(Smarket)
write.xlsx(Default, 'Data\Textbook_Data\Default.xlsx', row.names=FALSE)
# Learning more about the data set
?Smarket
# Generate the probabilyt function based on the fitted model for prediction
glm.probs <- predict(glm.fit,type="response")
# Logistic Regression
# Use the General Linear Model function to build the logistic regression model.
glm.fit <- glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,
data=Smarket,family=binomial)
# Print the summary of the fitted model
summary(glm.fit)
# Generate the probabilyt function based on the fitted model for prediction
glm.probs <- predict(glm.fit,type="response")
# Use the probability function to predict the first five observations in the data
# It will return the probability for going up the next day
glm.probs[1:5]
# Import ISLR library
require(ISLR)
library("xlsx")
names(Default)
?Default
write.xlsx(Default, 'Data\\Textbook_Data\\Default.xlsx', row.names=FALSE)
# Call out the Smarket data set
names(Smarket)
# print summary of the data set
summary(Smarket)
# Export dataset to csv file
write.csv(Smarket, 'Data\\Textbook_Data\\Smarket.csv', row.names=FALSE)
# Learning more about the data set
?Smarket
# Plot the pairs correlation of the data
pairs(Smarket,col=Smarket$Direction)
# Logistic Regression
# Use the General Linear Model function to build the logistic regression model.
glm.fit <- glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,
data=Smarket,family=binomial)
# Print the summary of the fitted model
summary(glm.fit)
# Generate the probabilyt function based on the fitted model for prediction
glm.probs <- predict(glm.fit,type="response")
# Use the probability function to predict the first five observations in the data
# It will return the probability for going up the next day
glm.probs[1:5]
# We can also use the ifelse() function to change the return values
# If the probabiliyt is over 50%, it will return "Up", else it will return "Down"
glm.pred <- ifelse(glm.probs>0.5,"Up","Down")
# Compare the return values to the previous
# As you observed, the probability function is nested to this new prediction function
glm.pred[1:5]
# We can attach the Smarket data set for convenience
attach(Smarket)
# Check the model prediction to the actual data
table(glm.pred,Direction)
# We can measure the accuracy of the model prediction by taking the mean of the correct results
mean(glm.pred == Direction)
# We can also check the error of the model prediction by reversing the measure
mean(glm.pred != Direction)
# For better accuracy check, we are going to split our data into training and testing sets
# Make training and testing set
# Training data will be the data with year before 2005
train <- Year<2005
# Use the glm() function to build the logistic regression model on the training data
glm.fit <- glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,
data=Smarket,family=binomial, subset=train)
summary(glm.fit)
# Create the probability function by using the fitted model
glm.probs <- predict(glm.fit,newdata=Smarket[!train,],type="response")
# Create the prediction function by defining the results with threshold 50% or above
glm.pred <- ifelse(glm.probs >0.5,"Up","Down")
# Compare the prediction with the testing data
Direction.2005 <- Smarket$Direction[!train]
# Present the comparison in a matrix table
table(glm.pred,Direction.2005)
# Calculate the accuracy for the prediction
mean(glm.pred == Direction.2005)
# We can also find the error rate for the prediction
mean(glm.pred != Direction.2005)
# In the previous model, we found that the lag1 and lag2 are the best predictors in the model
# Let's build a smaller model with just lag1 and lag2 for the logistic regression model
glm.fit <- glm(Direction~Lag1+Lag2,
data=Smarket,family=binomial, subset=train)
glm.probs <- predict(glm.fit,newdata=Smarket[!train,],type="response")
glm.pred <- ifelse(glm.probs >0.5,"Up","Down")
table(glm.pred,Direction.2005)
mean(glm.pred == Direction.2005)
mean(glm.pred != Direction.2005)
# We are going to use the Linear Discriminant Analysis
# The LDA requires the library MASS
require(MASS)
# Linear Discriminant Analysis
# Using lda() function for the linear discriminant analysis with lag1 and lag2
lda.fit <- lda(Direction~Lag1+Lag2,data=Smarket, subset=Year<2005)
# Check the model fit
lda.fit
# Plot the model fit
plot(lda.fit)
# Define the testing data year = 2005 for testing
Smarket.2005 <- subset(Smarket,Year==2005)
# Use the fitted model to make the prediction using the testing data
lda.pred <- predict(lda.fit,Smarket.2005)
# Predict the first 5 observations
lda.pred[1:5,]
# Predict the first 5 observations
lda.pred[1:5]
# Predict the first 5 observations
lda.pred[1:5,]
# Check the class in the prediction function
class(lda.pred)
# Show the prediction in a data frame
data.frame(lda.pred)[1:5,]
# Predict the first 5 observations
print(lda.pred[1:5,])
# Predict the first 5 observations
lda.pred[1:5]
# Predict the first 5 observations
lda.pred$class[1:5]
# Predict the first 5 observations
lda.pred$class[1:5,]
# Predict the first 5 observations
lda.pred$class[1:5]
# Show the prediction in a data frame
data.frame(lda.pred)[1:5,]
# Show the prediction in a data frame
data.frame(lda.pred)[1:5]
# Show the prediction in a data frame
data.frame(lda.pred)[1:5,]
# Predict the first 5 observations
lda.pred$class[1:5,]
# Predict the first 5 observations
lda.pred[1:5,]
# Check the class in the prediction function
class(lda.pred)
# Check the class in the prediction function
class(lda.pred)
# Show the prediction in a data frame
data.frame(lda.pred)[1:5,]
# Compare the prediction and testing data in a matrix table
table(lda.pred$class,Smarket.2005$Direction)
# Check the accuracy from the LDA model
mean(lda.pred$class==Smarket.2005$Direction)
# Quadratic Discriminant Analysis
# Using qda() function for the linear discriminant analysis with lag1 and lag2
qda.fit <- qda(Direction~Lag1+Lag2,data=Smarket, subset=Year<2005)
# Check the model fit
qda.fit
# Plot the model fit
plot(qda.fit)
# Plot the model fit
library(klaR)
# Plot the model fit
require(MASS)
library(klaR)
# Plot the model fit
install.packages(“klaR”)
# Plot the model fit
install.packages('klaR')
library(klaR)
partimat(Direction~Lag1+Lag2,data=Smarket,method="qda")
# Define the testing data year = 2005 for testing
Smarket.2005 <- subset(Smarket,Year==2005)
# Use the fitted model to make the prediction using the testing data
qda.pred <- predict(qda.fit,Smarket.2005)
# Predict the first 5 observations
qda.pred[1:5,]
# Predict the first 5 observations
qda.pred[1:5]
# Predict the first 5 observations
qda.pred$class[1:5]
# Check the class in the prediction function
class(qda.pred)
# Show the prediction in a data frame
data.frame(lda.pred)[1:5,]
# Predict the first 5 observations
qda.pred$class[1:5,:]
# Predict the first 5 observations
qda.pred[1:5,:]
# Predict the first 5 observations
qda.pred[1:5,]
print(qda.pred)
# Predict the first 5 observations
lda.pred$class[1:5]
# Show the prediction in a data frame
data.frame(lda.pred)[1:5,]
# Compare the prediction and testing data in a matrix table
table(qda.pred$class,Smarket.2005$Direction)
# Check the accuracy from the LDA model
mean(qda.pred$class==Smarket.2005$Direction)
# K-Nearest Neighbors
# The KNN method, we need to import library 'class'
library(class)
# Check the knn() function
?knn
# Attached the Smarket data set
attach(Smarket)
# Creating the lag variable, which includes lag1 and lag2
Xlag <- cbind(Lag1,Lag2)
# Create the training data for data before the year 2005
train <- Year<2005
# Use the knn() function to input the training set, testin set, and response variable (Direction)
# Note that we give the parameter k = 1, which defines the number of neighbor
knn.pred <- knn(Xlag[train,],Xlag[!train,],Direction[train],
k=1, prob=FALSE)
# Compare the prediction and the testing data in a matrix table
table(knn.pred,Direction[!train])
# Check the accuracy of the fitted model
mean(knn.pred == Direction[!train])
# Check the error rate of the fitted model
mean(knn.pred != Direction[!train])
# Once the package is installed, you can get access to the libraries within the package
library(MASS)
library(ISLR)
# Within the MASS library, there is a data set named "Boston".
# We will be using this data set for our exercise today.
# Boston is the source of Data
?Boston
?Default
library(MASS)
library(ISLR)
library(klaR)
library(ggplot2)
library(corrplot)
library(lmtest)
library(car)
library(pROC)
library(generalhoslem)
# Importing the Boston data set for linear regression analysis.
boston <- read.csv("data/Boston.csv", header=TRUE, na.string="?")
# Importing the Boston data set for linear regression analysis.
boston <- read.csv("./data/Boston.csv", header=TRUE, na.string="?")
# Setting the working directory for the project.
setwd("C:/Users/lokma/Desktop/SFSU_Training/SFSU_Faculty_Training")
# Importing the Boston data set for linear regression analysis.
boston <- read.csv("./data/Boston.csv", header=TRUE, na.string="?")
# Estimate the predicted effect of per capita crime rate on the median home value
# in Boston:
fit1 <- lm(medv~crim, data=boston)
summary(fit1)
# Multiple Linear Regression
# Estimate the predicted effect of the per capita crime rate, lower status
# of the population, and Charles River dummy on median home value in Boston:
fit2 <- lm(medv~crim+lstat+chas)
summary(fit2)
# If we want to use the same dataset over and over again, we can use
# function attach().
attach(boston)
# Multiple Linear Regression
# Estimate the predicted effect of the per capita crime rate, lower status
# of the population, and Charles River dummy on median home value in Boston:
fit2 <- lm(medv~crim+lstat+chas)
summary(fit2)
#Breusch-Pagan / Cook-Weisberg test for heteroskedasticity
bptest(fit2)
# Variance Inflation Factor test for multicollinearity
vif(fit2)
# Logistic Regression Model
# Import the data set Default for the logistic regression analysis.
default_data <- read.csv("data/Default.csv", header=TRUE, na.string="?")
attach(default_data)
# Estimate the log odds of Default using balance, income, and student:
fit4 <- glm(default~balance+income+student,
data=default_data, family=binomial)
summary(fit4)
# Store the predicted probability from the model.
glm.prob <- predict(fit4, type="response")
default_data$prob <- glm.prob
#Breusch-Pagan / Cook-Weisberg test for heteroskedasticity
bptest(fit4)
# Variance Inflation Factor test for multicollinearity
vif(fit4)
