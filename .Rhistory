# Create the prediction function by defining the results with threshold 50% or above
glm.pred <- ifelse(glm.probs >0.5,"Up","Down")
# Compare the prediction with the testing data
Direction.2005 <- Smarket$Direction[!train]
# Present the comparison in a matrix table
table(glm.pred,Direction.2005)
# Calculate the accuracy for the prediction
mean(glm.pred == Direction.2005)
# We can also find the error rate for the prediction
mean(glm.pred != Direction.2005)
# In the previous model, we found that the lag1 and lag2 are the best predictors in the model
# Let's build a smaller model with just lag1 and lag2 for the logistic regression model
glm.fit <- glm(Direction~Lag1+Lag2,
data=Smarket,family=binomial, subset=train)
glm.probs <- predict(glm.fit,newdata=Smarket[!train,],type="response")
glm.pred <- ifelse(glm.probs >0.5,"Up","Down")
table(glm.pred,Direction.2005)
mean(glm.pred == Direction.2005)
mean(glm.pred != Direction.2005)
# We are going to use the Linear Discriminant Analysis
# The LDA requires the library MASS
require(MASS)
# Linear Discriminant Analysis
# Using lda() function for the linear discriminant analysis with lag1 and lag2
lda.fit <- lda(Direction~Lag1+Lag2,data=Smarket, subset=Year<2005)
# Check the model fit
lda.fit
# Plot the model fit
plot(lda.fit)
# Define the testing data year = 2005 for testing
Smarket.2005 <- subset(Smarket,Year==2005)
# Use the fitted model to make the prediction using the testing data
lda.pred <- predict(lda.fit,Smarket.2005)
# Predict the first 5 observations
lda.pred[1:5,]
# Predict the first 5 observations
lda.pred[1:5]
# Predict the first 5 observations
lda.pred[1:5,]
# Check the class in the prediction function
class(lda.pred)
# Show the prediction in a data frame
data.frame(lda.pred)[1:5,]
# Predict the first 5 observations
print(lda.pred[1:5,])
# Predict the first 5 observations
lda.pred[1:5]
# Predict the first 5 observations
lda.pred$class[1:5]
# Predict the first 5 observations
lda.pred$class[1:5,]
# Predict the first 5 observations
lda.pred$class[1:5]
# Show the prediction in a data frame
data.frame(lda.pred)[1:5,]
# Show the prediction in a data frame
data.frame(lda.pred)[1:5]
# Show the prediction in a data frame
data.frame(lda.pred)[1:5,]
# Predict the first 5 observations
lda.pred$class[1:5,]
# Predict the first 5 observations
lda.pred[1:5,]
# Check the class in the prediction function
class(lda.pred)
# Check the class in the prediction function
class(lda.pred)
# Show the prediction in a data frame
data.frame(lda.pred)[1:5,]
# Compare the prediction and testing data in a matrix table
table(lda.pred$class,Smarket.2005$Direction)
# Check the accuracy from the LDA model
mean(lda.pred$class==Smarket.2005$Direction)
# Quadratic Discriminant Analysis
# Using qda() function for the linear discriminant analysis with lag1 and lag2
qda.fit <- qda(Direction~Lag1+Lag2,data=Smarket, subset=Year<2005)
# Check the model fit
qda.fit
# Plot the model fit
plot(qda.fit)
# Plot the model fit
library(klaR)
# Plot the model fit
require(MASS)
library(klaR)
# Plot the model fit
install.packages(“klaR”)
# Plot the model fit
install.packages('klaR')
library(klaR)
partimat(Direction~Lag1+Lag2,data=Smarket,method="qda")
# Define the testing data year = 2005 for testing
Smarket.2005 <- subset(Smarket,Year==2005)
# Use the fitted model to make the prediction using the testing data
qda.pred <- predict(qda.fit,Smarket.2005)
# Predict the first 5 observations
qda.pred[1:5,]
# Predict the first 5 observations
qda.pred[1:5]
# Predict the first 5 observations
qda.pred$class[1:5]
# Check the class in the prediction function
class(qda.pred)
# Show the prediction in a data frame
data.frame(lda.pred)[1:5,]
# Predict the first 5 observations
qda.pred$class[1:5,:]
# Predict the first 5 observations
qda.pred[1:5,:]
# Predict the first 5 observations
qda.pred[1:5,]
print(qda.pred)
# Predict the first 5 observations
lda.pred$class[1:5]
# Show the prediction in a data frame
data.frame(lda.pred)[1:5,]
# Compare the prediction and testing data in a matrix table
table(qda.pred$class,Smarket.2005$Direction)
# Check the accuracy from the LDA model
mean(qda.pred$class==Smarket.2005$Direction)
# K-Nearest Neighbors
# The KNN method, we need to import library 'class'
library(class)
# Check the knn() function
?knn
# Attached the Smarket data set
attach(Smarket)
# Creating the lag variable, which includes lag1 and lag2
Xlag <- cbind(Lag1,Lag2)
# Create the training data for data before the year 2005
train <- Year<2005
# Use the knn() function to input the training set, testin set, and response variable (Direction)
# Note that we give the parameter k = 1, which defines the number of neighbor
knn.pred <- knn(Xlag[train,],Xlag[!train,],Direction[train],
k=1, prob=FALSE)
# Compare the prediction and the testing data in a matrix table
table(knn.pred,Direction[!train])
# Check the accuracy of the fitted model
mean(knn.pred == Direction[!train])
# Check the error rate of the fitted model
mean(knn.pred != Direction[!train])
# Once the package is installed, you can get access to the libraries within the package
library(MASS)
library(ISLR)
# Within the MASS library, there is a data set named "Boston".
# We will be using this data set for our exercise today.
# Boston is the source of Data
?Boston
?Default
# Importing the Boston data set for linear regression analysis.
boston <- read.csv("data/Boston.csv",header = TRUE, na.string="?")
setwd("C:/Users/lokma/Desktop/SFSU_Training/SFSU_Faculty_Training")
# Importing the Boston data set for linear regression analysis.
boston <- read.csv("data/Boston.csv",header = TRUE, na.string="?")
# Getting the descriptive summary of the data.
summary(boston)
# Getting the descriptive summary for just one of the columns.
summary(boston$crim)
# No abbreviation can be used in R or Python because they are the name of
# the function written by the programmers.
# If we want to learn more about the function, we can use "?".
?summary
# Using all observation or a subset
summary(medv if crim > 5)
# Using all observation or a subset
summary(boston$medv[data$crim > 5])
# Using all observation or a subset
summary(boston$medv[data$crim > 5], basic=T)
# Using all observation or a subset
summary(boston$medv, data$crim > 5)
# Using all observation or a subset
summarise(boston$medv, data$crim > 5)
# Using all observation or a subset
Summarise(boston$medv, data$crim > 5)
# Using all observation or a subset
summary(boston$medv, data$crim > 10)
# Using all observation or a subset
summary(boston$medv, boston$crim > 10)
# Using all observation or a subset
summary(boston$crim > 10)
# Using all observation or a subset
summary(boston[boston$crim > 10]$medv, na.rm=TRUE)
# Using all observation or a subset
summary(boston$medv if(boston$crim > 10), na.rm=TRUE)
# Using all observation or a subset
summary(boston$medv if (boston$crim > 10), na.rm=TRUE)
# Using all observation or a subset
if(boston$crim > 10) summary(boston$medv, na.rm=TRUE)
# Using all observation or a subset
if(boston$crim > 10) {
summary(boston$medv, na.rm=TRUE)
}
# If we want to use the same dataset over and over again, we can use
# function attach().
attach(boston)
# Graphing Quantitative Data (Histogram)
# The dataset has a variable 'medv' which represents the median home's
# value.  Creating a historgram for it is easy:
ggplot(boston, aes(x=medv)) + geom_histogram()
library(ggplot2)
# Graphing Quantitative Data (Histogram)
# The dataset has a variable 'medv' which represents the median home's
# value.  Creating a historgram for it is easy:
ggplot(boston, aes(x=medv)) + geom_histogram()
# We can also add mean line on the graph.
p+ geom_value(aes(xintercept=mean(medv)),
color='blue', linetype='dashed', size=1)
# Graphing Quantitative Data (Histogram)
# The dataset has a variable 'medv' which represents the median home's
# value.  Creating a historgram for it is easy:
hist <- ggplot(boston, aes(x=medv)) + geom_histogram()
# We can also add mean line on the graph.
hist+ geom_value(aes(xintercept=mean(medv)),
color='blue', linetype='dashed', size=1)
# We can also add mean line on the graph.
hist+ geom_vline(aes(xintercept=mean(medv)),
color='blue', linetype='dashed', size=1)
# We can also plot the histogram with desity plot
ggplot(boston, aes(x=medv)) +
geom_histogram(aes(y=..density..), colour='black', fill='white') +
geom_density(alpha=0.2, fill="#FF6666")
# We can also plot the histogram with desity plot
ggplot(boston, aes(x=medv)) +
geom_histogram(aes(y=..Density..), colour='black', fill='white') +
geom_density(alpha=0.2, fill="#FF6666")
# We can also plot the histogram with desity plot
ggplot(boston, aes(x=medv)) +
geom_histogram(aes(y=..density..), colour='black', fill='white') +
geom_density(alpha=0.2, fill="#FF6666")
# We can also plot the histogram by groups.
ggplot(boston, aes(x=medv, color=student)) +
geomhistogram(fill="white")
# We can also plot the histogram by groups.
ggplot(boston, aes(x=medv, color=student)) +
geom_histogram(fill="white")
# We can also plot the histogram by groups.
ggplot(boston, aes(x=medv, color=student)) +
geom_histogram(fill="white")
# We can also plot the histogram by groups.
ggplot(boston, aes(x=medv, color=chas)) +
geom_histogram(fill="white")
# We can also plot the histogram by groups.
ggplot(boston, aes(x=medv, color=chas)) +
geom_histogram(fill="white")
View(boston)
View(boston)
?aes
# We can also plot the histogram by groups.
ggplot(boston, aes(x=medv, color=boston$chas)) +
geom_histogram(fill="white")
# We can also plot the histogram by groups.
ggplot(boston, aes(x=medv, color=boston$chas)) +
geom_histogram(fill="white")
# We can also plot the histogram by groups.
ggplot(boston, aes(x=medv, color=boston$chas)) +
geom_histogram(binwidth=0.02, fill="black")
# We can also plot the histogram by groups.
ggplot(boston, aes(x=medv, color=boston$chas)) +
geom_histogram(binwidth=0.02, fill="white")
# We can also plot the histogram by groups.
ggplot(boston, aes(x=medv, color=boston$chas)) +
geom_histogram(binwidth=0.2, fill="white")
# We can also plot the histogram by groups.
ggplot(boston, aes(x=medv, color=chas)) +
geom_histogram(binwidth=0.2, fill="white")
# We can also plot the histogram by groups.
ggplot(boston, aes(x=medv, color=chas, group=chas)) +
geom_histogram(binwidth=0.2, fill="white")
# We can also plot the histogram by groups.
ggplot(boston, aes(x=medv, color=chas, group=chas)) +
geom_histogram(binwidth=0.3, fill="white")
# We can also plot the histogram by groups.
ggplot(boston, aes(x=medv, color=chas, group=chas)) +
geom_histogram(binwidth=0.5, fill="white")
# We can also plot the histogram by groups.
ggplot(boston, aes(x=medv, color=chas, group=chas)) +
geom_histogram(binwidth=5, fill="white")
# We can also plot the histogram by groups.
ggplot(boston, aes(x=medv, color=chas, group=chas)) +
geom_histogram(binwidth=2, fill="white")
# We can also plot the histogram by groups.
ggplot(boston, aes(x=medv, color=chas, group=chas)) +
geom_histogram(binwidth=1, fill="white")
# Comparing Quantitative Data by Categories (Box Plot)
# Our dataset identifies whether a home is located nearby the river or not
# "chas". We can use a boxplot to compare nearby or none nearby river
# home value:
p <- ggplot(boston, aes(x=chas, y=medv)) +
geom_boxplot()
View(p)
View(p)
# Comparing Quantitative Data by Categories (Box Plot)
# Our dataset identifies whether a home is located nearby the river or not
# "chas". We can use a boxplot to compare nearby or none nearby river
# home value:
ggplot(boston, aes(x=chas, y=medv)) +
geom_boxplot()
# Note that "chas" is a dummy variable, but R is treating it as continues
# variable in the summary.  Therefore, we need to define it as categorical
# for later analysis.
boston$chas <- factor(boston$chas)
# We can also add mean line on the graph.
hist+ geom_vline(aes(xintercept=mean(medv)),
color='blue', linetype='dashed', size=1)
# We can also plot the histogram with desity plot
ggplot(boston, aes(x=medv)) +
geom_histogram(aes(y=..density..), colour='black', fill='white') +
geom_density(alpha=0.2, fill="#FF6666")
# We can also plot the histogram by groups.
ggplot(boston, aes(x=medv, color=chas, group=chas)) +
geom_histogram(binwidth=1, fill="white")
# Comparing Quantitative Data by Categories (Box Plot)
# Our dataset identifies whether a home is located nearby the river or not
# "chas". We can use a boxplot to compare nearby or none nearby river
# home value:
ggplot(boston, aes(x=chas, y=medv)) +
geom_boxplot()
# Plotting two quantitative varaibles against each other (Scatter Plot)
# Our data contains variables on median home value and per capita crime rate.
# A scatter plot for these two variables is easily created with:
ggplot(boston, aes(x=crim, y=medv)) +
geom_point(size=2, shape=5)
# Plotting two quantitative varaibles against each other (Scatter Plot)
# Our data contains variables on median home value and per capita crime rate.
# A scatter plot for these two variables is easily created with:
ggplot(boston, aes(x=crim, y=medv)) +
geom_point(size=2, shape=4)
# Plotting two quantitative varaibles against each other (Scatter Plot)
# Our data contains variables on median home value and per capita crime rate.
# A scatter plot for these two variables is easily created with:
ggplot(boston, aes(x=crim, y=medv)) +
geom_point(size=2, shape=1)
View(boston)
# Single Linear Regression Model
# Checking the correlation between variables
cor(boston)
# Single Linear Regression Model
# Checking the correlation between variables
cor("boston")
# Single Linear Regression Model
# Checking the correlation between variables
cor()
# Single Linear Regression Model
# Checking the correlation between variables
cor(medv, crim)
# Single Linear Regression Model
# Checking the correlation between variables
cor(boston, use="complete.obs")
library(corrplot)
install.packages('corrplot')
library(corrplot)
# Note that the cor() function can only take numeric value, but "chas" is
# a string (or factor).  We can change it back to numeric or create a new vector.
chas_riv <- unfactor(boston$chas)
# Note that the cor() function can only take numeric value, but "chas" is
# a string (or factor).  We can change it back to numeric or create a new vector.
chas_riv <- as.numeric(boston$chas)
# Note that the cor() function can only take numeric value, but "chas" is
# a string (or factor).  We can change it back to numeric or create a new vector.
chas_riv <- as.factor(boston$chas)
boston$chas <- as.numeric(boston$chas)
# Try again with the cor() function with boston dataset
cor(boston, use="complete.obs")
correlation
# Try again with the cor() function with boston dataset
correlation <- cor(boston, use="complete.obs")
correlation
# We can also plot the correlation with corrplot() function in R.
corrplot(correlation, type="upper", order="hclust", tl.col="black", tl.srt=45)
# Estimate the predicted effect of per capita crime rate on the median home value
# in Boston:
fit1 <- lm(medv~crim, data=boston)
summary(fit1)
# Plotting the regression line.
abline(fit1,col="red")
# Plotting the regression line.
abline(fit1,col="red")
# Graphing the correlation among variables in the dataset.
pairs(boston)
# Plotting the regression line.
plot(crim, medv, col="blue",
main="Median Home Value v Per Captia Crime Rate",
xlab="Per Capita Crime Rate", ylab="Median Home Value")
abline(fit1, col="red")
# Plot residuals against predicted value of home value
plot(predict(fit1), residuals(fit1))
# Multiple Linear Regression
# Estimate the predicted effect of the per capita crime rate, lower status
# of the population, and Charles River dummy on median home value in Boston:
fit2 <- lm(medv~crim+lstat+chas)
summary(fit2)
# Robust Tests
# Ramsey RESET test
resettest(fit2)
library(lmtest)
install.packages("lmtest")
library(lmtest)
# Robust Tests
# Ramsey RESET test
resettest(fit2)
#Breusch-Pagan / Cook-Weisberg test for heteroskedasticity
ols_test_breusch_pagan(fit2)
#Breusch-Pagan / Cook-Weisberg test for heteroskedasticity
bptest(fit2)
#Breusch-Pagan / Cook-Weisberg test for heteroskedasticity
httest(fit2)
# Variance Inflation Factor test for multicollinearity
vif
# Variance Inflation Factor test for multicollinearity
vif(fit2)
library(car)
# Variance Inflation Factor test for multicollinearity
vif(fit2)
# Logistic Regression Model
# Import the data set Default for the logistic regression analysis.
default <- read.csv("data/Default.csv", header=TRUE, na.string="?")
# Descriptive summary of the dataset.
summary(default)
str(default)
attach(default)
# Logistic Regression Model
# Import the data set Default for the logistic regression analysis.
default_data <- read.csv("data/Default.csv", header=TRUE, na.string="?")
# Descriptive summary of the dataset.
summary(default_data)
attach(default_data)
# Estimate the log odds of Default using the average balance that the
# customer has remaining on their credit card after making their monthy
# payment:
fit3 <- glm(default~balance, data=default_data, family=binomial)
summary(fit3)
# Estimate the log odds of Default using balance, income, and student:
fit4 <- glm(default~balance+income+student,
data=default_data, family=binomial)
summary(fit4)
library(pROC)
install.packages("pROC")
library(pROC)
# Evaluate the area under the ROC curve
prob <- predict(fit4, type=c('response'))
default_data$prob <- prob
g <- roc(default~prob, data=default_data)
plot(g)
# Calculate Hosmer-Lemeshow goodness-of-fit statistic
hoslem.test(fit4$default, fitted(fit4), g=10)
library(ResourceSelection)
install.packages("ResourceSelection")
library(ResourceSelection)
# Calculate Hosmer-Lemeshow goodness-of-fit statistic
hoslem.test(fit4$default, fitted(fit4), g=10)
print(fit4$default)
print(fitted(fit4))
# Calculate Hosmer-Lemeshow goodness-of-fit statistic
hoslem.test(default, fitted(fit4), g=10)
# Calculate Hosmer-Lemeshow goodness-of-fit statistic
hoslem.test(default, fitted(fit4), g=3)
# Calculate Hosmer-Lemeshow goodness-of-fit statistic
hoslem.test(fit4$default, fitted(fit4), g=3)
# Calculate Hosmer-Lemeshow goodness-of-fit statistic
h.test <- hoslem.test(fit4$default, fitted(fit4), g=3)
# Calculate Hosmer-Lemeshow goodness-of-fit statistic
h.test <- hoslem.test(default_data$default, fitted(fit4), g=3)
print(fit4$default)
# Calculate Hosmer-Lemeshow goodness-of-fit statistic
logitgof(default, fitted(fit4), g=10)
install.packages("generalhoslem")
library(logitgof)
library(generalhoslem)
# Calculate Hosmer-Lemeshow goodness-of-fit statistic
logitgof(default, fitted(fit4), g=10)
# Calculate Hosmer-Lemeshow goodness-of-fit statistic
logitgof(default_data$default, fitted(fit4), g=10)
# Confusion Matrix (Pr > 0.5)
glm.pred <- ifelse(probs > 0.5,"Yes","No")
# Confusion Matrix (Pr > 0.5)
glm.pred <- ifelse(prob > 0.5,"Yes","No")
table(glm.pred, default)
print(prob)
# Evaluate the area under the ROC curve
prob <- predict(fit4, type='response')
default_data$prob <- prob
g <- roc(default~prob, data=default_data)
plot(g)
View(default_data)
# Evaluate the area under the ROC curve
prob <- predict(fit4, type=c('response'))
default_data$prob <- prob
g <- roc(default~prob, data=default_data)
plot(g)
# Calculate Hosmer-Lemeshow goodness-of-fit statistic
# Note: this statistical test can be adjusted by number of groups 'g'
logitgof(default_data$default, fitted(fit4), g=10)
# Confusion Matrix (Pr > 0.5)
glm.pred <- ifelse(prob > 0.5,"Yes","No")
print(glm.pred)
# Estimate the log odds of Default using balance, income, and student:
fit4 <- glm(default~balance+income+student,
data=default_data, family=binomial)
summary(fit4)
predict(fit4)
predict(fit4, type="response")
default_data$prob <- glm.prob
# Store the predicted probability from the model.
glm.prob <- predict(fit4, type="response")
default_data$prob <- glm.prob
g <- roc(default~prob, data=default_data)
plot(g)
plot(g)
# Confusion Matrix (Pr > 0.5)
glm.pred <- ifelse(glm.prob > 0.5,"Yes","No")
table(glm.pred, default)
table(glm.pred, default_data$default)
