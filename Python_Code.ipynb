{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Command Line Usage\n",
    "\n",
    "For the purpose of this presentation, we focus on some basic descriptive statistic command and some common regression model technique. All examples are based on the datasets stored in the data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dependency\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing the current working directory\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Boston dataset for linear regression analysis.\n",
    "boston = pd.read_csv('./data/Boston.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first five row of the data.\n",
    "boston.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize the data set.\n",
    "boston.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize a subset of columns\n",
    "boston[[\"tax\", \"crim\"]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the type of the data\n",
    "boston.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# If we are not sure about what the function does, we can use the help() function or \"?\".\n",
    "help(print)\n",
    "?print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a subset of data for descriptive summary, for instand, only data with 'crim' over 5\n",
    "subset1 = boston.loc[boston['crim'] > 5, : ] \n",
    "\n",
    "# subset1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the statistical summary of the median home value in the subset of the data.\n",
    "subset1['medv'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose we are interested to create a subset with crim > 5 and chas = 1\n",
    "subset2 = boston.loc[(boston['crim']>5) & (boston['chas']==1), : ] \n",
    "\n",
    "# subset2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the statistical summary of the median home value in the subset of the data.\n",
    "subset2['medv'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Graph with Matplotly\n",
    "* Graphing correlation among variables (Pairs Plot)\n",
    "* Graphing quantitative Data (Histogram)\n",
    "* Graphing Quantitative Data with Categories (Box Plot)\n",
    "* Graphing Quantitative variables against each other (Scatter Plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphing correlation among variables\n",
    "sns.pairplot(boston)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our dataset has a variable \"medv\" which represents the median home's value. \n",
    "# Creating a histogram for it is easy:\n",
    "sns.distplot(boston['medv'], bins=10, kde=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our dataset identifies whether a home is located nearby the river or not \"chas\"\n",
    "# We can use a boxplot to compare nearby or none nearby river home value:\n",
    "sns.boxplot(x=boston[\"chas\"], y=boston[\"medv\"], data=boston)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our data contains variables on median home value and per capita crime rate.\n",
    "# A scatter plot for these two variables is easily created with:\n",
    "sns.scatterplot(x=boston[\"crim\"], y=boston[\"medv\"], data=boston)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Linear Regression Model\n",
    "There are two main stream statistics libraries are generally used in Python:\n",
    "* StatsModels: More general statistic library for statisticians and researchers because it provides some of the statictical elements that Scikit-Learn does not offer, such as adjusted R^2, AIC, BIC, etc.\n",
    "* Scikit-Learn: More for industry to use for building machine learning model because it has the capability to implement and create pipline building web application for clients use.  The downside is that it will not general a statistical summary like we usually seen in Stata or R.\n",
    "\n",
    "In this demo, we will mainly focus on using StatsModels library.  Note that there is no one saying one is better than the other one, but most of the data scientist will use SciKit-Learn library because of the popularity of machine leanring and AI building."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the correlation between variables.\n",
    "boston.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the X and y variable to pass into the model.\n",
    "# The reason we need to reshape the data dimension because SciKit-Learn only takes numpy array for modeling.\n",
    "X = boston['crim'].values.reshape(-1,1)\n",
    "y = boston[\"medv\"].values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate the predicted effect of per capita crime rate on the median home value in Boston:\n",
    "# We will first use SciKit-Learn library for this example.\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once we store the Linear Regression model, we can use the model to fit the data X and y\n",
    "model.fit(X, y)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The estimated parameters from the model.\n",
    "print('Weight coefficents: ', model.coef_)\n",
    "print('y-axis intercept: ', model.intercept_)\n",
    "\n",
    "# As you can see from this result, there is no statistical summary we can print from the model fit with SciKit-Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's consider using the StatsModel library for the same model specification.\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "model = sm.OLS(y, sm.add_constant(X))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the model and printing the summary\n",
    "fit1 = model.fit()\n",
    "print(fit1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the regression results, residual vs. fitted plot\n",
    "residuals = fit1.resid\n",
    "fitted = fit1.fittedvalues\n",
    "smoothed = lowess(residuals, fitted)\n",
    "\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "plt.rcParams[\"figure.figsize\"]=(8,7)\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(fitted, residuals, edgecolors='k', facecolors='none')\n",
    "ax.plot(smoothed[:, 0], smoothed[:, 1], color='r')\n",
    "ax.set_ylabel('Residuals')\n",
    "ax.set_xlabel('Fitted Values')\n",
    "ax.set_title('Residuals vs. Fitted')\n",
    "ax.plot([min(fitted), max(fitted)], [0,0], color='k', linestyle=':', alpha=0.3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the first 5 fitted values\n",
    "print(fitted[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the first 5 residual values\n",
    "print(residuals[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the data with the regression line.\n",
    "plt.scatter(X, y)\n",
    "plt.plot(X, fitted, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another option for plotting the regression line is to use Seaborn\n",
    "sns.lmplot(x='crim', y='medv', data=boston)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression\n",
    "We are going to mainly focus on StatsModels from now and on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate the predicted effect of the per capita crime rate, lower status \n",
    "# of the population, and Charles River dummy on median home value in Boston:\n",
    "\n",
    "# First we need to define the X and y variables in our model.\n",
    "X = boston[['crim', 'lstat', 'chas']]\n",
    "y = boston['medv']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the model with StatsModels OLS function\n",
    "model2 = sm.OLS(y, sm.add_constant(X))\n",
    "fit2 = model2.fit()\n",
    "print(fit2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust Tests\n",
    "# Ramsey RESET Test (Cannot find any useable function from StatsModels)\n",
    "# Breusch-Pagan / Cook-Weisberg test for Heteroskedasticity\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "from statsmodels.stats.diagnostic import het_white\n",
    "\n",
    "white_test = het_white(fit2.resid, fit2.model.exog)\n",
    "print(f\"LM Statistic: {white_test[0]}\")\n",
    "print(f\"p-value: {white_test[1]}\")\n",
    "\n",
    "bp_test = het_breuschpagan(fit2.resid, fit2.model.exog)\n",
    "print(f\"LM Statistic: {bp_test[0]}\")\n",
    "print(f\"p-value: {bp_test[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variance Inflation Factor (VIF) Test\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Create a dataframe to save the results\n",
    "X1 = sm.add_constant(X)\n",
    "vif = pd.DataFrame()\n",
    "vif[\"VIF Factor\"] = [variance_inflation_factor(X1.values, i) for i in range(X1.shape[1])]\n",
    "vif[\"features\"] = X1.columns\n",
    "\n",
    "# Inspect VIF factors\n",
    "vif.round(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Model\n",
    "Again, we are going to focus on using StatsModels for the logistic model because it has more statistic elements reported from the summary tabel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Default dataset for linear regression analysis.\n",
    "default_data = pd.read_csv('./data/Default.csv')\n",
    "\n",
    "# Print the first five rows of the data\n",
    "default_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive Summary of the data set\n",
    "default_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate the log odds of Default using the average balance that the customer has remaining on their credit card after making their monthly payment.\n",
    "# This time we are going to use the glm function with statsmodel, which is similar to R.\n",
    "import statsmodels.formula.api as smf\n",
    "model3 = smf.glm(formula='default~balance+income+student',\n",
    "                data=default_data,\n",
    "                family=sm.families.Binomial(link=sm.genmod.families.links.logit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting themodel\n",
    "fit3 = model3.fit()\n",
    "\n",
    "# Printing the summary table\n",
    "print(fit3.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the predicted probability from the model.\n",
    "ypred = fit3.predict(default_data[['student', 'balance', 'income']])\n",
    "print(ypred[0:10])\n",
    "y_predicted = []\n",
    "for i in ypred:\n",
    "    if i > 0.5:\n",
    "        y_predicted.append(\"No\")\n",
    "    if i <= 0.5:\n",
    "        y_predicted.append(\"Yes\")\n",
    "        \n",
    "print(y_predicted[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust Tests:\n",
    "# Ramsey RESET test, again not available in the documentation.\n",
    "# #Breusch-Pagan / Cook-Weisberg test for heteroskedasticity\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "from statsmodels.stats.diagnostic import het_white\n",
    "\n",
    "white_test = het_white(fit3.resid_response, fit3.model.exog)\n",
    "print(f\"WH: {white_test[0]}\")\n",
    "print(f\"p-value: {white_test[1]}\")\n",
    "\n",
    "bp_test = het_breuschpagan(fit3.resid_response, fit3.model.exog)\n",
    "print(f\"BP: {bp_test[0]}\")\n",
    "print(f\"p-value: {bp_test[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variance Inflation Factor (VIF) Test\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Create a dataframe to save the results\n",
    "df = pd.get_dummies(data=default_data, columns=['student'])\n",
    "X = df[['student_Yes', 'balance', 'income']]\n",
    "X1 = sm.add_constant(X)\n",
    "vif = pd.DataFrame()\n",
    "vif[\"VIF Factor\"] = [variance_inflation_factor(X1.values, i) for i in range(X1.shape[1])]\n",
    "vif[\"features\"] = X1.columns\n",
    "\n",
    "# Inspect VIF factors\n",
    "vif.round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "con_matrix = confusion_matrix(default_data[\"default\"], y_predicted)\n",
    "print(con_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or we can use the Pandas crosstab() function\n",
    "y_actu = pd.Series(default_data['default'], name='Actual')\n",
    "y_pred = pd.Series(y_predicted, name='Predicted')\n",
    "df_confusion = pd.crosstab(y_actu, y_pred)\n",
    "print(df_confusion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
